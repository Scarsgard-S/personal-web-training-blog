<!DOCTYPE HTML>
<html lang="ru">
	<head>
		<meta charset="UTF-8">
		<title>День из блога</title>
		<link rel="stylesheet" href="styles.css">
	</head>
	<body>
		<header>
			<nav>
				<a href="index.html">Вернуться ко всем записям</a>
			</nav>
		</header>
		<main>
			<article>
				<h1 id="beginning" style="color: #7D0057;">18-й день, <time datetime="2022-08-24"> 24 Августа</time></br>
					HTML Academy – Знакомство с HTML и CSS Часть 3: Ссылки и изображения</h1>
				<p class="section-day">
					На этом этапе всё стало интересней, появились теги: a, атрибуты href, ссылки на место 
					на текущей страницу, форматы изображений.
				</p>
				<p>Если хотите, то <a href="img/download-image.jpg" download rel="noopener">скачайте</a> красивую картинку, которую я для вас приготовил.</p>
				<p>Далее представлен большой текст для того чтобы проверить якорь.</p>
				<a href="#ending">Перейти к концу</a>
				<h3>Нейронная сеть</h3>
				<p class="long-text">
					Материал из Википедии — свободной энциклопедии
					Перейти к навигацииПерейти к поиску
					Эта статья о понятии в программировании; о сетях нервных клеток живых организмов см. Нервная сеть.
				</p>
				<p class="long-text">
					Схема простой нейросети. Зелёным цветом обозначены входные нейроны, голубым — скрытые нейроны, жёлтым — выходной нейрон
					Нейро́нная сеть[1] (также искусственная нейронная сеть, ИНС) — математическая модель, а также её программное или аппаратное воплощение, построенная по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток живого организма. Это понятие возникло при изучении процессов, протекающих в мозге, и при попытке смоделировать эти процессы. Первой такой попыткой были нейронные сети У. Маккалока и У. Питтса[2]. После разработки алгоритмов обучения получаемые модели стали использовать в практических целях: в задачах прогнозирования, для распознавания образов, в задачах управления и др.
				</p>
				<p class="long-text">
					ИНС представляет собой систему соединённых и взаимодействующих между собой простых процессоров (искусственных нейронов). Такие процессоры обычно довольно просты (особенно в сравнении с процессорами, используемыми в персональных компьютерах). Каждый процессор подобной сети имеет дело только с сигналами, которые он периодически получает, и сигналами, которые он периодически посылает другим процессорам. И, тем не менее, будучи соединёнными в достаточно большую сеть с управляемым взаимодействием, такие по отдельности простые процессоры вместе способны выполнять довольно сложные задачи.
				</p>
				<p class="long-text">
					С точки зрения машинного обучения, нейронная сеть представляет собой частный случай методов распознавания образов, дискриминантного анализа;
					С точки зрения математики, обучение нейронных сетей — это многопараметрическая задача нелинейной оптимизации;
					С точки зрения кибернетики, нейронная сеть используется в задачах адаптивного управления и как алгоритмы для робототехники;
					С точки зрения развития вычислительной техники и программирования, нейронная сеть — способ решения проблемы эффективного параллелизма[3];
					С точки зрения искусственного интеллекта, ИНС является основой философского течения коннекционизма и основным направлением в структурном подходе по изучению возможности построения (моделирования) естественного интеллекта с помощью компьютерных алгоритмов.
					Нейронные сети не программируются в привычном смысле этого слова, они обучаются[a]. Возможность обучения — одно из главных преимуществ нейронных сетей перед традиционными алгоритмами. Технически обучение заключается в нахождении коэффициентов связей между нейронами. В процессе обучения нейронная сеть способна выявлять сложные зависимости между входными данными и выходными, а также выполнять обобщение. Это значит, что в случае успешного обучения сеть сможет вернуть верный результат на основании данных, которые отсутствовали в обучающей выборке, а также неполных и/или «зашумленных», частично искажённых данных.
				</p>
				<h3>Хронология</h3>
				<p class="long-text">
					1943 — У. Маккалок и У. Питтс формализуют понятие нейронной сети в фундаментальной статье о логическом исчислении идей и нервной активности[2]. В начале своего сотрудничества с Питтсом Н. Винер предлагает ему вакуумные лампы в качестве средства для реализации эквивалентов нейронных сетей[5].
				</p>
				<p class="long-text">
					1948 — опубликована книга Н. Винера о кибернетике. Основной идеей стало представление сложных биологических процессов математическими моделями.
				1949 — Д. Хебб предлагает первый алгоритм обучения.
				</p>
				<p class="long-text">
					В 1958 Ф. Розенблатт изобретает однослойный перцептрон и демонстрирует его способность решать задачи классификации[6]. Перцептрон использовали для распознавания образов, прогнозирования погоды. К моменту изобретения перцептрона завершилось расхождение теоретических работ Маккалока с «кибернетикой» Винера; Маккалок и его последователи вышли из состава «Кибернетического клуба».
				</p>
				<p class="long-text" id="ending">
					В 1960 году Бернард Уидроу[en] совместно со своим студентом Хоффом на основе дельта-правила (формулы Уидроу) разработали Адалин, который сразу начал использоваться для задач предсказания и адаптивного управления. Адалин был построен на базе созданных ими же (Уидроу — Хоффом) новых элементах — мемисторах[7][8].
				</p>
				<a href="#beginning">Перейти к началу</a>

<!-- В 1963 году в Институте проблем передачи информации АН СССР. А. П. Петровым проводится исследование задач «трудных» для перцептрона[9]. На эту работу в области моделирования ИНС в СССР опирался М. М. Бонгарда в своей работе как «сравнительно небольшой переделкой алгоритма (перцептрона) исправить его недостатки»[10].
В 1969 году М. Минский публикует формальное доказательство ограниченности перцептрона и показывает, что он неспособен решать некоторые задачи (проблема «чётности» и «один в блоке»), связанные с инвариантностью представлений.
В 1972 году Т. Кохонен и Дж. Андерсон[en] независимо предлагают новый тип нейронных сетей, способных функционировать в качестве памяти[11].
В 1973 году Б. В. Хакимов предлагает нелинейную модель с синапсами на основе сплайнов и внедряет её для решения задач в медицине, геологии, экологии[12].
1974 — Пол Дж. Вербос[13] и Галушкин А. И.[14] одновременно изобретают алгоритм обратного распространения ошибки для обучения многослойных перцептронов[15].
1975 — Фукусима[en] представляет когнитрон — самоорганизующуюся сеть, предназначенную для инвариантного распознавания образов, но это достигается только при помощи запоминания практически всех состояний образа.
1982 — Дж. Хопфилд показал, что нейронная сеть с обратными связями может представлять собой систему, минимизирующую энергию (сеть Хопфилда). Кохоненом представлены модели сети, обучающейся без учителя (нейронная сеть Кохонена), решающей задачи кластеризации, визуализации данных (самоорганизующаяся карта Кохонена) и другие задачи предварительного анализа данных.
1986 — Дэвидом И. Румельхартом, Дж. Е. Хинтоном и Рональдом Дж. Вильямсом[16], а также независимо и одновременно С. И. Барцевым и В. А. Охониным[17], переоткрыт и развит метод обратного распространения ошибки.
2007 — Джеффри Хинтоном в университете Торонто созданы алгоритмы глубокого обучения многослойных нейронных сетей. Хинтон при обучении нижних слоёв сети использовал ограниченную машину Больцмана (RBM — Restricted Boltzmann Machine). По Хинтону необходимо использовать много примеров распознаваемых образов (например, множество лиц людей на разных фонах). После обучения получается готовое быстро работающее приложение, способное решать конкретную задачу (например, осуществлять поиск лиц на изображении). -->
			</article>
		</main>
	</body>
</html>
